# QLoRA Fine-tuning for Verbal Autopsy

This directory contains scripts for fine-tuning a Llama 3:8B model on verbal autopsy data using QLoRA (Quantized Low-Rank Adaptation).

## Overview

QLoRA enables efficient fine-tuning of large language models by:
- Using 4-bit quantization to reduce memory usage
- Applying Low-Rank Adaptation (LoRA) for parameter-efficient training
- Maintaining model quality while requiring significantly less GPU memory

## Files

- `finetune_qlora.py` - Main fine-tuning script
- `qlora_config.py` - Configuration management
- `inference_qlora.py` - Inference script for testing fine-tuned models
- `setup_qlora.sh` - Environment setup script
- `requirements_qlora.txt` - Python dependencies
- `train.jsonl` - Training data (generated by `prepare_training_data.py`)
- `test.jsonl` - Test data (generated by `prepare_training_data.py`)

## Setup

### 1. Environment Setup

```bash
# Make setup script executable
chmod +x setup_qlora.sh

# Run setup (creates virtual environment and installs dependencies)
./setup_qlora.sh

# Activate the environment
source .venv_qlora/bin/activate
```

### 2. Manual Installation (Alternative)

```bash
# Create virtual environment
python -m venv .venv_qlora
source .venv_qlora/bin/activate

# Install PyTorch with CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install requirements
pip install -r requirements_qlora.txt

# Optional: Install flash attention for faster training
pip install flash-attn --no-build-isolation
```

## Usage

### 1. Generate Configuration Files

```bash
python qlora_config.py
```

This creates several configuration files:
- `config_default.json` - Default settings
- `config_quick_test.json` - For quick testing (1 epoch)
- `config_production.json` - For production training (5 epochs)
- `config_memory_optimized.json` - For limited GPU memory

### 2. Run Fine-tuning

```bash
# Basic training with default settings
python finetune_qlora.py

# On SLURM cluster with GPU
srun --reservation gpu -w n20 --gres=gpu:1 python finetune_qlora.py
```

### 3. Monitor Training

Training logs and metrics are saved to:
- `models/va_qlora_TIMESTAMP/` - Model checkpoints
- `finetune_qlora_TIMESTAMP.log` - Training logs
- TensorBoard logs in the output directory

```bash
# View TensorBoard
tensorboard --logdir models/va_qlora_TIMESTAMP/
```

### 4. Run Inference

```bash
# Test fine-tuned model
python inference_qlora.py \
    --adapter_path models/va_qlora_TIMESTAMP \
    --test_file test.jsonl \
    --num_cases 10 \
    --output_file results.json
```

## Training Configuration

Key parameters that can be adjusted:

### Model Settings
- `model_name_or_path`: Base model (default: "meta-llama/Meta-Llama-3-8B-Instruct")
- `max_length`: Maximum sequence length (default: 4096)

### LoRA Settings
- `lora_r`: LoRA rank (default: 16, higher = more parameters)
- `lora_alpha`: LoRA scaling parameter (default: 32)
- `lora_dropout`: Dropout rate (default: 0.1)
- `target_modules`: Which layers to apply LoRA to

### Training Settings
- `num_train_epochs`: Number of training epochs (default: 3)
- `learning_rate`: Learning rate (default: 2e-4)
- `per_device_train_batch_size`: Batch size per GPU (default: 1)
- `gradient_accumulation_steps`: Effective batch size multiplier (default: 4)

## Memory Requirements

Approximate GPU memory usage:
- **Llama 3:8B with QLoRA**: ~12-16 GB VRAM
- **With batch_size=1, gradient_accumulation=4**: ~14 GB VRAM
- **Memory optimized config**: ~10-12 GB VRAM

For lower memory GPUs, adjust:
- Reduce `max_length` to 2048 or 1024
- Increase `gradient_accumulation_steps`
- Ensure `per_device_train_batch_size=1`

## Model Output Format

The fine-tuned model generates JSON responses in this format:

```json
{
  "ID": "CASE123",
  "CAUSE_SHORT": "Diarrhoeal diseases",
  "SCHEME_CODE": "01.04",
  "CONFIDENCE": "85"
}
```

## Troubleshooting

### CUDA Out of Memory
- Reduce `max_length`
- Reduce `per_device_train_batch_size` to 1
- Increase `gradient_accumulation_steps`
- Use `config_memory_optimized.json`

### Flash Attention Issues
- Set `use_flash_attention=False` in config
- Install flash-attn: `pip install flash-attn --no-build-isolation`

### Import Errors
- Ensure all requirements are installed
- Check CUDA version compatibility
- Try reinstalling transformers: `pip install transformers --upgrade`

### Slow Training
- Enable flash attention
- Use `group_by_length=True`
- Ensure using GPU: check `torch.cuda.is_available()`

## Expected Training Time

On a single GPU:
- **Quick test (1 epoch)**: ~30-60 minutes
- **Default (3 epochs)**: ~2-4 hours  
- **Production (5 epochs)**: ~4-8 hours

**GPU-specific estimates for Llama 3:8B QLoRA:**
- **NVIDIA L4 (24GB)**: ~45-90 min/epoch (good memory, moderate speed)
- **NVIDIA A100 (40/80GB)**: ~20-40 min/epoch (fastest)
- **NVIDIA V100 (32GB)**: ~60-120 min/epoch (older architecture)
- **RTX 4090/3090 (24GB)**: ~30-60 min/epoch (consumer cards)

Times vary based on:
- GPU type and memory bandwidth
- Sequence length (longer = slower)
- Batch size and gradient accumulation
- Dataset size and complexity

## Model Evaluation

After training, evaluate your model using:

1. **Automatic metrics**: Use the evaluation scripts in the parent directory
2. **Manual inspection**: Review sample predictions in `inference_results.json`
3. **Accuracy metrics**: Compare predicted vs. ground truth scheme codes

## Next Steps

1. **Hyperparameter tuning**: Experiment with different LoRA ranks, learning rates
2. **Data augmentation**: Generate more training examples
3. **Multi-GPU training**: Scale to multiple GPUs for faster training
4. **Model deployment**: Convert to GGUF format for efficient inference

## Support

For issues:
1. Check the log files for detailed error messages
2. Verify GPU memory usage with `nvidia-smi`
3. Ensure all dependencies are correctly installed
4. Review the HuggingFace documentation for transformers and PEFT